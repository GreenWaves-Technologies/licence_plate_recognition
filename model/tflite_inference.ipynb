{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from PIL import Image\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input details:  [{'name': 'input', 'index': 63, 'shape': array([ 1, 24, 94,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0078125, 128)}]\n",
      "Output details:  [{'name': 'Squeeze', 'index': 53, 'shape': array([ 1, 88, 71], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.5366795659065247, 128)}]\n"
     ]
    }
   ],
   "source": [
    "image_file = \"/home/marco-gwt/GWT/OCR/model/lp_recognition/LPRNet_china_no_tile_quantized/0m_1_resized.png\"\n",
    "image_grayscale = False\n",
    "input_grayscale = False\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "model_path = \"/home/marco-gwt/GWT/OCR/model/lp_recognition/LPRNet_china_no_tile_quantized/china_ocr.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Input details: \", input_details)\n",
    "print(\"Output details: \", output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_in = Image.open(image_file)\n",
    "if input_grayscale and not image_grayscale:\n",
    "    img_in = img_in.convert('L')\n",
    "#img_in = img_in.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]))\n",
    "input_array = np.array(img_in, dtype=np.uint8)\n",
    "input_array = np.reshape(input_array, input_details[0]['shape'])\n",
    "interpreter.set_tensor(input_details[0]['index'], input_array)\n",
    "interpreter.invoke()\n",
    "\n",
    "tens_out = {'input': input_array}\n",
    "for i in range(len(output_details)):\n",
    "    tens_out.update({output_details[i]['name']: interpreter.get_tensor(output_details[i]['index'])})\n",
    "\n",
    "#json_dict = {}\n",
    "#for k, v in tens_out.items():\n",
    "#    json_dict.update({k: v.tolist()})\n",
    "#with open('tensors_out.json', 'w') as f:\n",
    "#    json.dump(json_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [[[[ -62.  -62.  -62.]\n",
      "   [ -61.  -61.  -61.]\n",
      "   [ -58.  -58.  -58.]\n",
      "   ...\n",
      "   [ -91.  -91.  -91.]\n",
      "   [ -94.  -94.  -94.]\n",
      "   [ -73.  -73.  -73.]]\n",
      "\n",
      "  [[ -70.  -70.  -70.]\n",
      "   [ -71.  -71.  -71.]\n",
      "   [ -71.  -71.  -71.]\n",
      "   ...\n",
      "   [ -88.  -88.  -88.]\n",
      "   [ -92.  -92.  -92.]\n",
      "   [ -77.  -77.  -77.]]\n",
      "\n",
      "  [[ -95.  -95.  -95.]\n",
      "   [ -97.  -97.  -97.]\n",
      "   [ -98.  -98.  -98.]\n",
      "   ...\n",
      "   [ -84.  -84.  -84.]\n",
      "   [ -86.  -86.  -86.]\n",
      "   [ -80.  -80.  -80.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-100. -100. -100.]\n",
      "   [-100. -100. -100.]\n",
      "   [ -99.  -99.  -99.]\n",
      "   ...\n",
      "   [ -87.  -87.  -87.]\n",
      "   [ -86.  -86.  -86.]\n",
      "   [-106. -106. -106.]]\n",
      "\n",
      "  [[-106. -106. -106.]\n",
      "   [-107. -107. -107.]\n",
      "   [-107. -107. -107.]\n",
      "   ...\n",
      "   [ -89.  -89.  -89.]\n",
      "   [ -88.  -88.  -88.]\n",
      "   [-107. -107. -107.]]\n",
      "\n",
      "  [[-108. -108. -108.]\n",
      "   [-108. -108. -108.]\n",
      "   [-110. -110. -110.]\n",
      "   ...\n",
      "   [ -92.  -92.  -92.]\n",
      "   [ -89.  -89.  -89.]\n",
      "   [-104. -104. -104.]]]]\n",
      "Squeeze [[[-16. -17. -14. ... -16. -17. -18.]\n",
      "  [-16. -15. -14. ... -13. -12.  -5.]\n",
      "  [-16. -16. -14. ... -13. -13.  -2.]\n",
      "  ...\n",
      "  [-16. -12. -13. ... -12. -15.   7.]\n",
      "  [-19. -14. -15. ... -14. -18.   3.]\n",
      "  [-15. -13. -13. ... -14. -15.  -7.]]]\n"
     ]
    }
   ],
   "source": [
    "for k, v in tens_out.items():\n",
    "    print(k, v.astype(np.float32)-128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 88, 71)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens_out['Squeeze'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70]\n",
      "['<Beijing>']\n"
     ]
    }
   ],
   "source": [
    "out = tens_out['Squeeze'][0]\n",
    "out_char_codes = [np.argmax(out[i]) for i in range(out.shape[0])]\n",
    "\n",
    "char_dict = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '<Anhui>': 10, '<Beijing>': 11, '<Chongqing>': 12, '<Fujian>': 13, '<Gansu>': 14, '<Guangdong>': 15, '<Guangxi>': 16, '<Guizhou>': 17, '<Hainan>': 18, '<Hebei>': 19, '<Heilongjiang>': 20, '<Henan>': 21, '<HongKong>': 22, '<Hubei>': 23, '<Hunan>': 24, '<InnerMongolia>': 25, '<Jiangsu>': 26, '<Jiangxi>': 27, '<Jilin>': 28, '<Liaoning>': 29, '<Macau>': 30, '<Ningxia>': 31, '<Qinghai>': 32, '<Shaanxi>': 33, '<Shandong>': 34, '<Shanghai>': 35, '<Shanxi>': 36, '<Sichuan>': 37, '<Tianjin>': 38, '<Tibet>': 39, '<Xinjiang>': 40, '<Yunnan>': 41, '<Zhejiang>': 42, '<police>': 43, 'A': 44, 'B': 45, 'C': 46, 'D': 47, 'E': 48, 'F': 49, 'G': 50, 'H': 51, 'I': 52, 'J': 53, 'K': 54, 'L': 55, 'M': 56, 'N': 57, 'O': 58, 'P': 59, 'Q': 60, 'R': 61, 'S': 62, 'T': 63, 'U': 64, 'V': 65, 'W': 66, 'X': 67, 'Y': 68, 'Z': 69, '_': 70}\n",
    "\n",
    "out_char = []\n",
    "for i, char_code in enumerate(out_char_codes):\n",
    "    if char_code == 70:\n",
    "        continue\n",
    "    for k, v in char_dict.items():\n",
    "        if char_code == v:\n",
    "            out_char.append(k)\n",
    "            continue\n",
    "print(out_char_codes)\n",
    "print(out_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
